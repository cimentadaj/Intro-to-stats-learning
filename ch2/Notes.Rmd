---
title: "Notes on chapter 2 - Introduction to Statistical Learning"
output: html_notebook
---

The session started out introducing simple linear regression. It touched on what dependent/target/response variables are and what explanatory variables. They didn't go deep into the math but they did explain something which was interesting to me. They showed a scatterplot like this one:

```{r, echo = F}
set.seed(1)
x <- log(rnorm(1000, 5))
y <- rnorm(1000, 5) * x

qplot(x, y, geom = "point") + geom_smooth()
```

Where there's a linear relationship between x and y.  Actually, y is a function of x. If we wanted to predict the value of Y at any given x value we would simply look at the value in the regression slope. So for example, 

```{r, echo = F}
set.seed(1)
x <- log(rnorm(1000, 5))
y <- rnorm(1000, 5) * x

qplot(x, y, geom = "point") + geom_smooth() + geom_vline(xintercept = 1.0, colour = "red") + geom_hline(yintercept = 5, colour = "black") + scale_y_continuous(breaks = 2:18)
```

To obtain the mean Y when X is equal to 1.0, the regression slope estimates the mean for all values positioned in 1.0. Which gives us a 5. But what happens when there are no values in the X variable? Like for example in this part of the regression slope: 
```{r, echo = F}
set.seed(1)
x <- log(rnorm(1000, 5))
y <- rnorm(1000, 5) * x

qplot(x, y, geom = "point") + geom_smooth() + geom_vline(xintercept = 0.8, colour = "red") + geom_hline(yintercept = 4, colour = "black") + scale_y_continuous(breaks = 2:18) + scale_x_continuous(breaks = 0.8:3)
```

In a case like this one, Trevor Hastie explains that the regression slope relaxes the mean estimation and estimates a 'nearest neighbor mean'. In simple terms, the mean is calculated within a certain boundarie, including as a minimum 10% of the data points in x.

```{r, echo = F}
set.seed(1)
x <- log(rnorm(1000, 5))
y <- rnorm(1000, 5) * x

qplot(x, y, geom = "point") + geom_smooth() + geom_vline(xintercept = 0.75, colour = "red", linetype = "dashed") + geom_vline(xintercept = 0.95, colour = "red", linetype = "dashed") + scale_y_continuous(breaks = 2:18) + scale_x_continuous(breaks = 0.8:3)
```

As Trevor explains in the second lecture, nearest neighbor works fine when you have one, or maximum two covariates, simply because the distance between points is small. Once you include more covariates, the graph adds more dimensions for each covariate. This procedure throws in more distance for each dot as they're being relocated with the new values of new covariates. The problem is that the two red lines from above become wider and wider, as they need to cover 10% of the data points. This throws in biased estimations for prediction. Secondly, whenever you have a few data points in the data set, you'll have another bias problem, as the dotted red lines will have to be wider and wider apart until they can cover 10% of the data.

To sum up, neareast neighbor works good when you have reasonable number of data points( I suppose this is arbitrary and should be expected upon) and when you have only 1 or 2 covariates.

They call this the _course of dimensionality_.

Whenever you enocunter a problem such as this one, they briefly touch on a couple of methods which are better suited like thin-plate-spline.  The problem is that regression-type of methods tend to be less flexible but easier to interprete. Thin-plate-spline methods are extremely flexible, up to the point of fitting the data closely to the correct function, but are nearly impossible to interpret. This topic would be touched in the next chapters.

Flexible means that can fit the data better and better:

```{r}
set.seed(2131)
x <- rnorm(100)
y <- rnorm(100) + x^3
qplot(x, y, geom = "point") + geom_abline(colour = "red") +  geom_smooth(colour = "brown", span = 0.05, se = F) + geom_smooth(stat = "smooth", se = F)
```
Here you can see the flexibility of different models. Starting with the red regression slope, which fits the data not so good, followed by a smoothed line which fits the pattern reasonably well, and finally the smoothed line with a small span which practically touches upon almost all dots.

As I still don't know how to compute it, if we estimated the MSE for each of these models, the MSE of the linear regression would be the highest, the middle smooth would be the best and the flexible smooth line would be smallest. However, overfitting the model is also negative for accuracy. As they show in the session, one should pick another chunk of the data and fit the flexible smooth and you'll see how the MSE increases.

In a scenarion like this one:
```{r, echo = F}
set.seed(2131)
x <- rnorm(100)
y <- x + rnorm(100, sd = 0.1)
qplot(x, y, geom = "point") + geom_abline(colour = "red") + geom_smooth(se = F) + geom_smooth(colour = "brown", span = 0.05, se = F)

```
In an scenario like this one, the flexibility is actually not necessary, as the data does not require more flexibility than the regression estimations. The MSE of the regression and the second model are virtually the same, with the last method being not that efficient. In this example the flexibility-accuracy trade off is not present, as the simplest model is flexible enough to get the best predictions.

```{r, echo = F}
set.seed(21311)
x <- rnorm(100)
y <- x^2 + rnorm(100, sd = 0.3)
qplot(x, y, geom = "point") + geom_abline(colour = "red") + geom_smooth(se = F) + geom_smooth(colour = "brown", se = F, span = 0.05)
```
In this scenario, although not perfectly depicted, the MSE of the flexible model is better than the linear model, even when applying the model in a subset of the training data.




