---
title: "Applied exercises"
output: html_notebook
---

# Applied

8.
(a)

```{r}
library(ISLR)
mod1 <- lm(mpg ~ horsepower, Auto)
summary(mod1)
```
i. There is a significant relationship. That is, the relationship is different from 0.

ii. For a one unit increase in horsepower the expected miles per gallon decreases by about 0.15. In addition, the R squared is of 60% suggesting that the horsepower explains mpg quite well.

iii. negative

iv.
```{r}
predict(mod1, newdata = data.frame(horsepower = 98))
# 24.46 mpg

# prediction and confidence interval
upper <- summary(mod1)$coef[, 1] + 1.96 * summary(mod1)$coef[, 2]
lower <- summary(mod1)$coef[, 1] - 1.96 * summary(mod1)$coef[, 2]

cbind(lower, upper)
```

(b)
```{r}
with(Auto, plot(mpg ~ horsepower)); abline(mod1, col = "red")
```
(c)
```{r}
plot(mod1)
```
Only two significant outliers, perhaps rerunning without those two outliers would be informative. The residuals are not entirely normally distributed but nothing really dramatic. The most worrisome thing is the residuals vs fitted values as there seems to be a clearn pattern. As fitted values increase the residual decrease. But after a certain threshold it starts to increase.

9.
(a)
```{r}
plot(Auto)
```

(b)
```{r}
cor(Auto[!(names(Auto) %in% "name")])
```

(c)
```{r}
mod2 <- lm(mpg ~ . -name, Auto)
summary(mod2)
```

i. Yes as the F statistic is significant suggesting that At least ONE predictord is significantly related to mpg. Some predictors, however, don't have a significant relationship while others do.

ii. `displacement`, `weight`, `year` and `origin`.

iii. That as the years increase, the mpg is increasing by about 0.75.

(d)
```{r}
plot(mod2)
```
The outlier plot suggests there is an outlier present, namely row 14, and its leverage is quite high. The residuals vs fitted is less severe than as before, but still it shows a pattern that as fitted values increase residuals become more dispersed. The residuals seem to be more normal than before.

(e)
```{r}
summary(lm(mpg ~ origin*cylinders + year*weight, Auto))
```
Yes, for example year and weight shows a significant interaction.

(f)
```{r}
summary(lm(mpg ~ I(log(horsepower)) + weight + I(weight^2), Auto))
```

10.
(a)
```{r}
mod3 <- lm(Sales ~ Price + Urban + US, Carseats)
```

(b)
```{r}
summary(mod3)$coef
```
As Price increases, sales go down. Urban has about -0.02 less sales than rural and US has about 1.20 more sales than it's counterpart.

(c)
`y = b0 + Price * -0.054 + Urban(Yes) * -0.02 + US(Yes) * 1.20`

(d)
For all except Urban.

(e)
```{r}
mod4 <- lm(Sales ~ Price + US, Carseats)
summary(mod4)$coef
```

(f)
```{r}
summary(mod3)$r.sq
summary(mod4)$r.sq
# They're practically the same. Let's test it formally with an anove test

anova(mod4, mod3)

The additional covariate doesn't add anything to the model.
```

(g)
```{r}
upper <- summary(mod4)$coef[, 1] + 1.96 * summary(mod4)$coef[, 2]
lower <- summary(mod4)$coef[, 1] - 1.96 * summary(mod4)$coef[, 2]

cbind(lower, upper)
```

(h)
```{r}
plot(mod4)
```
There are some small outliers but nothing dramatic in terms of leverage, at least marked by the plot. Although one could exclude the highest point on the right part of the plot just to confirm.

11.

```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
```

(a)
```{r}
mod5 <- lm(y ~ x - 1)
summary(mod5)$coef
```
It seems that the estimated increase is different from zero because the standard error `0.11 * 1.96 == 0.21` and that `+/-` 2.1 doesn't include a zero.

(b)
```{r}
mod6 <- lm(x ~ y - 1)
summary(mod6)$coef
```
Same t statistic and p value but different estimate and standard error. 

(c)
The estimates and therefore the SE's are different because substantively you're predicting another thing. But in terms of significance the relationship being tested is exactly the same; whether on predicts or not the other.

(d)
```{r}
(sqrt(length(y) - 1)) * sum(x * y) / sqrt(sum(x^2) * sum(y^2) - sum(x * y)^2) # is the same as:
summary(mod6)$coef[, 3]
```
(e)
Every calculation done in (d) is either addition or multiplication involving x and y. If you swap the places, the result is not changed, so the t value is the same for both.

(f)
```{r}
summary(lm(y ~ x))$coef[ , 3, drop = F]
summary(lm(x ~ y))$coef[ , 3, drop = F]
# Exactly the same.

```

12.
(a)

The condition happens whe the sum of squares of x and y are equal. In other terms, when both variables are exactly the same, in terms of addition.

(b)
```{r}
x <- rnorm(100)
y <- rnorm(100)

coef(lm(y ~ x))
coef(lm(x ~ y))
```

(c)
```{r}
x <- rnorm(100)
y <- -sample(x, 100)

coef(lm(y ~ x - 1))
coef(lm(x ~ y - 1))
```

13.
(a)
```{r}
set.seed(1)

x <- rnorm(100)
```

(b)
```{r}
eps <- rnorm(100, 0, 0.25)
```

(c)
```{r}
y <- -1 + 0.5*x + eps
length(y)
# b0 = -1 and b1 == 0.5
```

(d)
```{r}
plot(x, y)
```
There'sa fairly linear relationship, as it would be expected from the formula of y.

(e)
```{r}
mod1 <- lm(y ~ x)
summary(mod1)
```
The coefficients are practically the same. In fact a difference between the two would be probably be insignificant.

(f)
```{r}
plot(x, y)
abline(lm(y ~ x))
```

(g)
```{r}
summary(lm(y ~ poly(x, 2)))
```
The adjusted R2 slightly increased and the RSE slightly decreased meaning that the term improved the model. Despite this, the term is insignificant.

(h)
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0, 0.12)
y <- -1 + 0.5*x + eps
mod2 <- lm(y ~ x)
summary(mod2)
```

The estimation suggests that the model is explained prettly well. This is evident given that y is a function of X and epsilon, which has NO variation at all.

(i)
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0, 0.50)
y <- -1 + 0.5*x + eps
mod3 <- lm(y ~ x)
summary(mod3)
```

The same model with more variation naturally has less explained variance. The adjusted R2 is smaller and the RSE is higher.

(j)
```{r}
confint(mod1)
confint(mod2)
confint(mod3)
```
The second model, the one with the less unexplained variance, has the smallest CI's, followed the by the third model, the one with the highest variance unexplained, with the biggest CI's. For the first model the size of the CI's are in between the 2nd and 3rd model. Everything as expected.

14.
(a)
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/100
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100, sd = 1)
coef(lm(y ~ x1 + x2))
```
The coefficients are b0 = 2.1 (similar yo the 2 in the function form), b1 = -1.75(far away from the 2 in the functional form) and b2  = 7.39(highly overestimated from the 0.3 in the functional form)

(b)
```{r}
cor(x1, x2)
plot(x1, x2)
abline(lm(x2 ~ x1))
```

(c)
```{r}
summary(lm(y ~ x1 + x2))
```
These coefficients are pretty far from the true coefficients. One telling thing from the output is that both coefficients have a reasonable size but their SE's are huge. This means that both coefficients are insignificantly different from 0.

(d)
```{r}
summary(lm(y ~ x1))
```
The results are highly significant and pretty close to the true coefficient

(e)
```{r}
summary(lm(y ~ x2))
```
This one the coefficient is highly overestimated but it's still significant.

(f)
The results do contradict each other because separetely they are significant but when pooled together insignificant. This contradiction is a clue in understanding that there's something the is increasing their standard errors.

(g)
```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)

summary(lm(y ~ x1 +  x2))
```
From the histograms of each variables x2 seemed like the one where the point would have the biggest leverage. In fact, the coefficient increased by a lot. On the other variable it didn't have an important effect. We can look at it's importance by plotting the distribution.
```{r}
hist(x1)
```
Adding a `1` did not disturb the more or less 'uniform' distribution.

15.