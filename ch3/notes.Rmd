---
title: "Notes"
output: html_notebook
---

# Notes

Introduction to linear regression.

Linear regression looks to estimate a linear relationship between a response variable and one or more predictors. It is estiamted through a method called least squares which I will explain in detail below.

```{r}
library(arm)

set.seed(1)
n = 100
a = 2
b1 = 5
x1 = rnorm(n, 5, sd = 0.2)
epsilon <- rnorm(n)

y = a + b1 * x1 + epsilon

mod1 <- lm(y ~ x1)
display(mod1, detail = T)
plot(x1, y)
abline(a = 1.99, b = 4.99, col = "blue", lwd = 2)
segments(x1, y, x1, predict(mod1), col = "red")
```

These are the smallest deviations from the prediction that conforms a regression line. We estimate these deviations be subtracting each point from the predicted value. To estimate the residual some of squares or RSS, we take each of this deviations and add them all together. As you can see, some of these deviations will be negative like for example the first point in roughly the coordinates [4.5, 24.7]. For that we squared all deviations and add them together. Let's give it a try:
```{r}
yhat = predict(mod1) # predicted values
RSS = sum((y - yhat)^2) # the difference from the predicted values and the actual values
all.equal(RSS, sum((mod1$residuals)^2)) # comparing our calculations from the model estimations
```
There you go, our estimations are correct.

We can estimate other parameters from the linear regression manually. For example, the regression coefficient. The formula is outlined as the correlation between x and y divided by the standard deviation between y and x.

```{r}
ourbeta <- (cor(y, x1) * sd(y)/sd(x1))
all.equal(ourbeta, unname(coef(mod1)[2]))
```
Also, correct. Let's move to the standard error:

```{r}
e <- sqrt(sum(mod1$residuals^2)/ (length(y) - 2)) # This is the residual standard deviation
ssx <- sum((x1 - mean(x1))^2) # And these are the sum of deviations from the mean of x

# Divide them both while taking the sqrt of ssx and you get the SE.
se <- e/sqrt(ssx); se
display(mod1) # compare

```
After this, confidence intervals are pretty straight forward.

```{r}
upper <- b1 + 2 * se; upper
lower <- b1 - 2 * se; lower

confint(mod1)
# I don't know why they don't round up to the exact numbers.

# Also the t statistic
b1/se
```

The second video talked about how to compute the R squared and how to do hypothesis testing. What specifically tests the hypothesis testing and how it's related to the confidence intervals.

How do we compute the R squared? It's pretty straight forward. Its the total variance explained divided by the total variance. It can be interpreted as the % of variance that is explained out of the total amount of variance. It measured the strength of the relationship between the predictors and the response variable. Don't confuse this with statistical significance. Statistical significance means that the predictor is different from zero and not that there's a sizable effect or that there's a strong relationship.

```{r}
total_var = sum((y - mean(y))^2) # total variance in y
var_not_explained = sum((y - yhat)^2) # total variance not explained (deviations of y from predictions)

1 - (var_not_explained/total_var) # divide them and we get the % of variance NOT explained. subtract a 1 and get the % of variance explained
summary(mod1)$r.squared # compare to this
```

In the third video we talk about multiple regression and how it would be ideal if all predictors are uncorrelated. That would mean that it is not important to include them in a model simply because the estimates will not change if they're being controlled for.

```{r}
n <- 100
x1 <- rnorm(n, 5)
x2 <- rnorm(n, 10)

a = 3
b1 = 2
b2 = 7
e <- rnorm(n)

y = a + b1 * x1 + b2 * x2 + e

display(lm(y ~ x1), detail = T) # un correlated models
display(lm(y ~ x1 + x2), detail = T) # un correlated models
# Coefficients hardly change

# Let's redefine:
x1 <- rnorm(n, 5)
x2 <- x1 * rnorm(n, 10)
y = a + b1 * x1 + b2 * x2 + e

display(lm(y ~ x1), detail = T) # correlated models
# See the biased coefficient?

display(lm(y ~ x1 + x2), detail = T) # correlated models
# See how it comes down?

# Those are the perils of correlated coefficients
```
But besides this problem, there's another thing: holding fixed other coefficients does not resemble reality. In the third video they talk about an example that exemplifies the problem pretty well. Suppose that Y is the change you have in your pocket, x1 is the number of coins and x2 is the number of pennies. x1 and x2 are inherently correlated.

```{r}
coins = 1:10
pennies = c(0, rep(1, 9))
cor(coins, pennies) # the more coins, the more pennies, obviously.

change = coins * pennies * rnorm(10, sd = 100) # change is the combination of coins and pennies
display(lm(change ~ coins + pennies))
```
Here we say that holding the # of pennies constant, an increase in coins yields more change. But in practice when coins increase, pennies almost ALWAYS increase. So be careful in interpreting this: this only means that regardless of the amount of pennies, the more coins the more change you have.

In the fourth video they begin by positing four questions related to the usefulness of regression models. First, they ask whether the model is useful in explaining a certain response variable. For that we use the F statistic. Why the F statistic and not look at the significance levels of each variable? Because under the 95% threshold, some variables might be significant simply due to chance. For example, when you ahve 100 predictors, just by chance we expect 5 of them to be signifianct when they're actually not! The F statistics accounts for that as it adjusts for the number predictors in the model.

The F statistic can be calculated as

```{r}
mod1 <- lm(y ~ x1)
TSS <- sum((y - mean(y))^2) # total sum of squares or total variance
RSS <- sum((y - predict(mod1))^2)
p <- 1 # number of predictors
n <- length(y)

f_stat <- ((TSS - RSS) / p) / (RSS/(n - p - 1))
all.equal(f_stat, unname(summary(mod1)$fstatistic[1]))
```
A problem with regression is knowing which variables to pick. In social sciences this is often no a problem because theory plays a small role but when dealing with massive data sets, variables are all correlated and are difficult to identify as important. That's where we use a method called forward selection. We start with a null model. Then we fit p models, each one with a response and a p variable. From all these models we pick the one with the less RSS, so the one that better explains the response. We include that p in the null model and being the process again with the remaining p variables. 

The algorithm will stop whenever a certain threshold has been met, like for example when the remaining variables are all insignificant or simply when the RSS is below or above an arbitrary threshold.

Another method is the backward selection. Contrary to the forward selection, you start with a model with all p variables. The model excludes the variable with the highest p value and refits the model without this variable. It continues to do so until a threshold is met, as with the forward selection.

The application of these algorithms will be seen in next chapters.

Up next the discuss how to interpret dummy variables:
```{r}
set.seed(4)
height <- rnorm(100, 5, sd = 0.5)
male <- rbinom(100, 1, ifelse(height > 5.5, 0.8, 0.1))

display(lm(height ~ male), detail = T)

```

This shows, in simple terms, that males have a heigh of about .52 higher than women, which is about 4.93. When more categories are included the results are interpreted the same way: always in reference to the intercept which is the baseline category.

Interactions and Non-linearities

```{r}
Adv <- read.csv("http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv", header = T)
```


